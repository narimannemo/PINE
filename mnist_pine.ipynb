{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ZPLVgFyn9wMM"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.summary API due to missing TensorBoard installation.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os\n",
    "import numpy as np\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, ZeroPadding2D, GlobalAveragePooling2D, UpSampling2D, Cropping2D\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PINE():\n",
    "  model_name = \"pine_mnist\"     # name for checkpoint\n",
    "  dataset_name = \"mnist\"\n",
    "\n",
    "  def __init__(self, batch_size, dataset_name):\n",
    "    \n",
    "      # Input shape\n",
    "      self.img_rows = 28\n",
    "      self.img_cols = 28\n",
    "      self.channels = 1\n",
    "      self.y_dim = 10\n",
    "      self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
    "      self.latent_dim = 100 \n",
    "      self.batch_size = batch_size\n",
    "      self.learning_rate_main_model = 0.0001\n",
    "      self.learning_rate_interpreter = 0.0001\n",
    "      self.checkpoint_dir = 'checkpoint'\n",
    "      self.dataset_name = dataset_name\n",
    " \n",
    "      # Build and compile the interpreter\n",
    "      self.interpreter = self.build_interpreter()\n",
    "\n",
    "      # Build the mian model\n",
    "      self.main_model = self.build_main_model()\n",
    "\n",
    "\n",
    "\n",
    "###################### SAVE and Load Process ##########################\n",
    "  @property\n",
    "  def model_dir(self):\n",
    "      return \"{}_{}_{}\".format(\n",
    "          self.model_name, self.dataset_name,\n",
    "          self.batch_size)\n",
    "      \n",
    "  def save(self, checkpoint_dir, step):\n",
    "      checkpoint_dir = os.path.join(checkpoint_dir, self.model_dir, self.model_name)\n",
    "\n",
    "      if not os.path.exists(checkpoint_dir):\n",
    "          os.makedirs(checkpoint_dir)\n",
    "\n",
    "      self.saver.save(self.sess,os.path.join(checkpoint_dir, self.model_name+'.model'), global_step=step)      \n",
    "  def load(self, checkpoint_dir):\n",
    "    import re\n",
    "    print(\" [*] Reading checkpoints...\")\n",
    "    checkpoint_dir = os.path.join(checkpoint_dir, self.model_dir, self.model_name)\n",
    "\n",
    "    ckpt = tf.train.get_checkpoint_state(checkpoint_dir)\n",
    "    if ckpt and ckpt.model_checkpoint_path:\n",
    "        ckpt_name = os.path.basename(ckpt.model_checkpoint_path)\n",
    "        self.saver.restore(self.sess, os.path.join(checkpoint_dir, ckpt_name))\n",
    "        counter = int(next(re.finditer(\"(\\d+)(?!.*\\d)\",ckpt_name)).group(0))\n",
    "        print(\" [*] Success to read {}\".format(ckpt_name))\n",
    "        return True, counter\n",
    "    else:\n",
    "        print(\" [*] Failed to find a checkpoint\")\n",
    "        return False, 0    \n",
    "#########################################################################\n",
    "  def build_main_model(self):\n",
    "    \n",
    "    #    ___________\n",
    "    #   /           \\\n",
    "    #  / MAIN  MODEL \\\n",
    "    # /_______________\\\n",
    "    # model: https://github.com/yashk2810/MNIST-Keras/blob/master/Notebook/MNIST_keras_CNN-99.55%25.ipynb\n",
    "    \n",
    "    \n",
    "    imgs = tf.keras.Input(shape=(28, 28, 1), name=\"img\")\n",
    "    x = tf.keras.layers.Conv2D(32, 3, activation=\"relu\")(imgs)\n",
    "    x = BatchNormalization(axis=-1)(x)\n",
    "    x = tf.keras.layers.Conv2D(32, 3, activation=\"relu\")(x)\n",
    "    x = tf.keras.layers.MaxPooling2D(2)(x)\n",
    "    x = BatchNormalization(axis=-1)(x)\n",
    "    x = tf.keras.layers.Conv2D(64, 3, activation=\"relu\")(x)\n",
    "    x = BatchNormalization(axis=-1)(x)\n",
    "    x = tf.keras.layers.Conv2D(64, 3, activation=\"relu\")(x)\n",
    "\n",
    "    x = tf.keras.layers.MaxPooling2D(2)(x)\n",
    "    x = tf.keras.layers.Flatten()(x)\n",
    "\n",
    "    # Fully connected layer\n",
    "    x = BatchNormalization()(x)\n",
    "    out_logit = tf.keras.layers.Dense(512, activation=\"relu\")(x)\n",
    "    x = BatchNormalization()(out_logit)\n",
    "    x = tf.keras.layers.Dropout(0.2)(x)\n",
    "    out = tf.keras.layers.Dense(10, activation=\"softmax\")(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs = imgs, outputs = out)\n",
    "\n",
    "    print(model.summary())\n",
    "    return model\n",
    "\n",
    "  def build_interpreter(self):\n",
    "    # _________________\n",
    "    # \\               /\n",
    "    #  \\             /\n",
    "    #   \\           /\n",
    "    #    INTERPRETER\n",
    "    #   /           \\\n",
    "    #  /             \\\n",
    "    # /_______________\\\n",
    "\n",
    "    # model = https://github.com/nathanhubens/Autoencoders/blob/master/Autoencoders.ipynb\n",
    "\n",
    "    # # Encoder\n",
    "    encoder_input = tf.keras.Input(shape=(28, 28, 1), name=\"img\")\n",
    "    x = tf.keras.layers.Conv2D(16, 3, activation=\"relu\", padding='same')(encoder_input)\n",
    "    x = tf.keras.layers.MaxPooling2D(2, padding='same')(x)\n",
    "    x = tf.keras.layers.Conv2D(8, 3, activation=\"relu\", padding='same')(x)\n",
    "    x = tf.keras.layers.MaxPooling2D(2, padding='same')(x)\n",
    "    x = tf.keras.layers.Conv2D(8, 3, activation=\"relu\", padding='same')(x)\n",
    "    x = tf.keras.layers.MaxPooling2D(2, padding='same')(x)\n",
    "\n",
    "    # # Decoder\n",
    "    x = tf.keras.layers.Conv2D(8, 3, activation=\"relu\", padding='same')(x)\n",
    "    x = tf.keras.layers.UpSampling2D(2)(x)\n",
    "    x = tf.keras.layers.Conv2D(8, 3, activation=\"relu\", padding='same')(x)\n",
    "    x = tf.keras.layers.UpSampling2D(2)(x)\n",
    "    x = tf.keras.layers.Conv2D(16, 3, activation=\"relu\")(x)\n",
    "    x = tf.keras.layers.UpSampling2D(2)(x)\n",
    "    out = tf.keras.layers.Conv2D(1, 3, activation=\"sigmoid\", padding='same')(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs = encoder_input, outputs = out)\n",
    "\n",
    "\n",
    "    print(model.summary())\n",
    "\n",
    "    return model\n",
    "\n",
    "  def main_model_loss(self, true, pred):\n",
    "\n",
    "    self.true = true\n",
    "    self.pred = pred\n",
    "    self.CatCrossEnt = tf.keras.losses.CategoricalCrossentropy()\n",
    "    def loss(true, pred):\n",
    "      return self.CatCrossEnt(true,pred)\n",
    "\n",
    "    return loss\n",
    "\n",
    "  \n",
    "  def train(self, epochs, batch_size=128, save_interval=50):\n",
    "\n",
    "        #################################################### \n",
    "        #                                ________________  #\n",
    "        #         ___________           \\               /  #\n",
    "        #        /           \\           \\             /   #\n",
    "        # Train / MAIN  MODEL \\           \\           /    #\n",
    "        #      /_______________\\           INTERPRETER     #\n",
    "        #                                 /           \\    #\n",
    "        #                                /             \\   #\n",
    "        #                               /_______________\\  #\n",
    "        ####################################################\n",
    "\n",
    "    # restore check-point if it exits\n",
    "    could_load, checkpoint_counter = self.load(self.checkpoint_dir)\n",
    "    if could_load:\n",
    "        start_epoch = (int)(checkpoint_counter / self.num_batches)\n",
    "        start_batch_id = checkpoint_counter - start_epoch * self.num_batches\n",
    "        counter = checkpoint_counter\n",
    "        print(\" [*] Load SUCCESS\")\n",
    "    else:\n",
    "        start_epoch = 0\n",
    "        start_batch_id = 0\n",
    "        counter = 1\n",
    "        print(\" [!] Load failed...\")\n",
    "    \n",
    "    self.epochs = epochs\n",
    "\n",
    "    # Load the dataset\n",
    "    (X_train,y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "    # Rescale -1 to 1\n",
    "    X_train = X_train / 127.5 - 1.\n",
    "    X_train = np.expand_dims(X_train, axis=3)\n",
    "\n",
    "\n",
    "    X_test = X_test / 127.5 - 1.\n",
    "    X_test = np.expand_dims(X_test, axis=3)\n",
    "\n",
    "    y_vec = np.zeros((len(y_train), 10), dtype=np.float)\n",
    "    for i, label in enumerate(y_train):\n",
    "        y_vec[i, y_train[i]] = 1.0\n",
    "\n",
    "    y_vec_test = np.zeros((len(y_test), 10), dtype=np.float)\n",
    "    for i, label in enumerate(y_test):\n",
    "        y_vec_test[i, y_test[i]] = 1.0\n",
    "\n",
    "    # Adversarial ground truths\n",
    "    valid = np.ones((batch_size, 1))\n",
    "    fake = np.zeros((batch_size, 1))\n",
    "    opt_interpreter = tf.keras.optimizers.Adam(self.learning_rate_interpreter)\n",
    "    opt_main_model = tf.keras.optimizers.Adam(self.learning_rate_main_model)\n",
    "    \n",
    "\n",
    "    start_batch_id = 0\n",
    "    self.num_batches = len(X_train) // self.batch_size    \n",
    "    for epoch in range(self.epochs):\n",
    "      for idx in range(start_batch_id, self.num_batches):\n",
    "        imgs = X_train[idx*self.batch_size:(idx+1)*self.batch_size]\n",
    "        self.y = y_vec[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "\n",
    "        # Iterate over the batches of a dataset.\n",
    "        # Open a GradientTape.\n",
    "        with tf.GradientTape(persistent= True) as tape:\n",
    "\n",
    "        # Train the interpreter (real classified as ones and generated as zeros)\n",
    "          ints = self.interpreter(imgs, training=True)\n",
    "\n",
    "          out_int = self.main_model(ints, training=True)\n",
    "\n",
    "          out_img = self.main_model(imgs, training=True)\n",
    "\n",
    "\n",
    "          # Get gradients of loss wrt the weights.\n",
    "          CatCrossEnt = tf.keras.losses.CategoricalCrossentropy()\n",
    "          loss_eval = CatCrossEnt(self.y, out_img)\n",
    "          main_model_grads = tape.gradient(loss_eval, self.main_model.trainable_weights)\n",
    "          out_int = self.main_model(ints)\n",
    "          int_error = tf.sqrt(2 * tf.nn.l2_loss(out_img - out_int)) / self.batch_size        \n",
    "          l1 = tf.dtypes.cast(int_error, tf.float32)\n",
    "          l2 = tf.dtypes.cast(CatCrossEnt(self.y, out_int), tf.float32)\n",
    "          out_sqrt = tf.keras.backend.sqrt(out_int)\n",
    "          sumi = tf.keras.backend.sum(out_sqrt)**2        \n",
    "          l3 = tf.dtypes.cast(0.0002*(sumi), tf.float32)\n",
    "          self.interpreter_loss = l1 + l2 + l3\n",
    "          interpreter_grads = tape.gradient(self.interpreter_loss, self.interpreter.trainable_weights)\n",
    "\n",
    "\n",
    "          # Update the weights of the model.\n",
    "          \n",
    "          opt_interpreter.apply_gradients(zip(interpreter_grads, self.interpreter.trainable_weights))\n",
    "          opt_main_model.apply_gradients(zip(main_model_grads, self.main_model.trainable_weights))\n",
    "\n",
    "\n",
    "        self.main_model.save(self.checkpoint_dir)\n",
    "        self.interpreter.save(self.checkpoint_dir)\n",
    "\n",
    "    self.main_model.save(self.checkpoint_dir)\n",
    "    self.interpreter.save(self.checkpoint_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    pine = PINE(batch_size=32, dataset_name=\"mnist\")\n",
    "    pine.train(epochs=5, save_interval=50)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPfWNSNeBPuqT6cEommP/qe",
   "collapsed_sections": [],
   "name": "Untitled1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
