{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.summary API due to missing TensorBoard installation.\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "import os\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "from tensorflow.keras import backend\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "from tensorflow.keras.models import Sequential \n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.utils import np_utils\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, ZeroPadding2D, GlobalAveragePooling2D\n",
    "from keras.layers.advanced_activations import LeakyReLU \n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.utils.generic_utils import get_custom_objects\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from ops import *\n",
    "from utils import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PINE(object):\n",
    "    model_name = \"PINE\"     # name for checkpoint\n",
    "\n",
    "    def __init__(self, sess, epoch, batch_size, dataset_name, checkpoint_dir):\n",
    "        self.sess = sess\n",
    "        self.dataset_name = dataset_name\n",
    "        self.checkpoint_dir = checkpoint_dir\n",
    "        self.epoch = epoch\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        if dataset_name == 'mnist':\n",
    "            # parameters\n",
    "            self.input_height = 28\n",
    "            self.input_width = 28\n",
    "            self.output_height = 28\n",
    "            self.output_width = 28\n",
    "      \n",
    "            self.y_dim = 10        \n",
    "            self.c_dim = 1\n",
    "\n",
    "            # train\n",
    "            self.learning_rate = 0.0001\n",
    "            self.beta1 = 0.5\n",
    "\n",
    "            # test\n",
    "            self.sample_num = 64  \n",
    "            self.len_discrete_code = 10\n",
    "\n",
    "            # load mnist\n",
    "            self.data_X, self.data_y = load_mnist(self.dataset_name)\n",
    "\n",
    "            # get number of batches for a single epoch\n",
    "            self.num_batches = len(self.data_X) // self.batch_size\n",
    "            self.kcc = tf.keras.losses.CategoricalCrossentropy()\n",
    "\n",
    "        elif dataset_name == 'cifar10':\n",
    "            # parameters\n",
    "            self.input_height = 32\n",
    "            self.input_width = 32\n",
    "            self.output_height = 32\n",
    "            self.output_width = 32\n",
    "      \n",
    "            self.y_dim = 10        \n",
    "            self.c_dim = 3\n",
    "\n",
    "            # Loss terms coefficients\n",
    "            self.c1 = 10000\n",
    "            self.c2 = 10000\n",
    "            self.c3 = 1000000\n",
    "            self.c4 = 1000\n",
    "            # train\n",
    "            self.learning_rate = 0.0001\n",
    "            self.beta1 = 0.5\n",
    "\n",
    "            # test\n",
    "            self.sample_num = 64  \n",
    "\n",
    "            # load mnist\n",
    "            self.data_X, self.data_X_test, self.data_y, self.data_y_test = load_cifar10(self.dataset_name)\n",
    "\n",
    "            # get number of batches for a single epoch\n",
    "            self.num_batches = len(self.data_X) // self.batch_size\n",
    "            self.kcc = tf.keras.losses.CategoricalCrossentropy()            \n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        \n",
    "#            ___________\n",
    "#           /           \\\n",
    "#          / MAIN  MODEL \\\n",
    "#         /_______________\\        \n",
    "\n",
    "\n",
    "        \n",
    "    def main_model(self, x, is_training=True, reuse=False):\n",
    "\n",
    "        with tf.compat.v1.variable_scope(\"main_model\", reuse=reuse):    \n",
    "\n",
    "            net = lrelu(coinv2d(x, 64, 4, 4, 2, 2, name='mm_conv1'))\n",
    "            net = lrelu(bn(coinv2d(net, 128, 4, 4, 2, 2, name='mm_conv2'), is_training=is_training, scope='mm_bn2'))\n",
    "            net = tf.reshape(net, [self.batch_size, -1])\n",
    "            net = lrelu(bn(linear(net, 1024, scope='mm_fc3'), is_training=is_training, scope='mm_bn3'))\n",
    "            out_logit = linear(net, 10, scope='mm_fc4')\n",
    "            out = tf.nn.softmax(out_logit)\n",
    "\n",
    "            return out, out_logit\n",
    "        \n",
    "        \n",
    "#          _________________\n",
    "#          \\               /\n",
    "#           \\             /\n",
    "#            \\           /\n",
    "#             INTERPRETER\n",
    "#            /           \\\n",
    "#           /             \\\n",
    "#          /_______________\\\n",
    "                \n",
    "\n",
    "    def interpreter(self, x, is_training=True, reuse=False):\n",
    "        \n",
    "        with tf.compat.v1.variable_scope(\"interpreter\", reuse=reuse):\n",
    "\n",
    "            net = tf.nn.relu(coinv2d(x, 64, 4, 4, 2, 2, name='int_conv1'))\n",
    "            net = tf.reshape(net, [self.batch_size, -1])\n",
    "            code = (linear(net, 32, scope='int_fc6')) # bn and relu are excluded since code is used in pullaway_loss\n",
    "            net = tf.nn.relu(bn(linear(code, 64 * 14 * 14, scope='int_fc3'), is_training=is_training, scope='int_bn3'))\n",
    "            net = tf.reshape(net, [self.batch_size, 14, 14, 64])\n",
    "            out = tf.nn.sigmoid(deconv2d(net, [self.batch_size, 28, 28, 1], 4, 4, 2, 2, name='int_dc5'))\n",
    "\n",
    "            # recon loss\n",
    "            recon_error = tf.sqrt(2 * tf.nn.l2_loss(out - x)) / self.batch_size\n",
    "            return out, recon_error, code\n",
    "\n",
    "        \n",
    "    def build_pine(self):\n",
    "        # some parameters\n",
    "        image_dims = [self.input_height, self.input_width, self.c_dim]\n",
    "        bs = self.batch_size\n",
    "\n",
    "        \"\"\" Graph Input \"\"\"\n",
    "        # images\n",
    "        self.inputs = tf.compat.v1.placeholder(tf.float32, [bs] + image_dims, name='real_images')\n",
    "\n",
    "        # labels\n",
    "        self.y = tf.compat.v1.placeholder(tf.float32, [bs, self.y_dim], name='y')\n",
    "\n",
    "\n",
    "        \"\"\" Loss Function \"\"\"\n",
    "\n",
    "\n",
    "        tafsir, tafsir_err, code = self.interpreter(self.inputs, is_training=True)\n",
    "        out_tafsir, out_logit_tafsir = self.main_model(tafsir, is_training=True)\n",
    "        out_real, out_logit_real = self.main_model(self.inputs, is_training=True, reuse= True)        \n",
    "\n",
    "\n",
    "\n",
    "        self.mm_loss = self.kcc(out_real,self.y)\n",
    "        out_sqrt = tf.keras.backend.sqrt(tafsir)\n",
    "        sumi = tf.keras.backend.sum(out_sqrt)**2\n",
    "        self.int_loss = 10000*tafsir_err + 10000*self.kcc(out_tafsir, self.y) + sumi / 1000000 \n",
    "\n",
    "\n",
    "        \"\"\" Training \"\"\"\n",
    "\n",
    "        t_vars = tf.compat.v1.trainable_variables()\n",
    "        int_vars = [var for var in t_vars if 'int_' in var.name]\n",
    "        mm_vars = [var for var in t_vars if 'mm_' in var.name]\n",
    "  \n",
    "\n",
    "        # optimizers\n",
    "        with tf.control_dependencies(tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.UPDATE_OPS)):\n",
    "\n",
    "            self.int_optim = tf.compat.v1.train.AdamOptimizer(self.learning_rate * 5, beta1=self.beta1) \\\n",
    "            .minimize(self.int_loss, var_list=int_vars)\n",
    "            self.mm_optim = tf.compat.v1.train.AdamOptimizer(self.learning_rate * 5, beta1=self.beta1) \\\n",
    "            .minimize(self.mm_loss, var_list=mm_vars)\n",
    "\n",
    "\n",
    "        \"\"\"\" Testing \"\"\"\n",
    "        # for test\n",
    "        self.tafsir_images = self.interpreter(self.inputs, is_training=False, reuse=True)\n",
    "        \"\"\" Summary \"\"\"\n",
    "        int_loss_sum = tf.compat.v1.summary.scalar(\"int_loss\", self.int_loss)\n",
    "        mm_loss_sum = tf.compat.v1.summary.scalar(\"mm_loss\", self.mm_loss)\n",
    "\n",
    "        self.int_sum = tf.compat.v1.summary.merge([int_loss_sum])\n",
    "        self.mm_sum = tf.compat.v1.summary.merge([mm_loss_sum])\n",
    "        #################################################### \n",
    "        #                                ________________  #\n",
    "        #    ___________                \\               /  #\n",
    "        #   /           \\    Parallel    \\             /   #\n",
    "        #  / MAIN  MODEL \\      ||        \\           /    #\n",
    "        # /_______________\\  Training      INTERPRETER     #\n",
    "        #                                 /           \\    #\n",
    "        #                                /             \\   #\n",
    "        #                               /_______________\\  #\n",
    "        ####################################################           \n",
    "    def train(self):\n",
    "\n",
    "        # initialize all variables\n",
    "        tf.compat.v1.global_variables_initializer().run()\n",
    "\n",
    "        # graph inputs for visualize training results\n",
    "        self.test_codes = self.data_y[0:self.batch_size]\n",
    "        self.sample_input = self.data_X[0:self.batch_size]\n",
    "\n",
    "        # saver to save model\n",
    "        self.saver = tf.compat.v1.train.Saver()\n",
    "\n",
    "        # restore check-point if it exits\n",
    "        could_load, checkpoint_counter = self.load(self.checkpoint_dir)\n",
    "        if could_load:\n",
    "            start_epoch = (int)(checkpoint_counter / self.num_batches)\n",
    "            start_batch_id = checkpoint_counter - start_epoch * self.num_batches\n",
    "            counter = checkpoint_counter\n",
    "            print(\" [i] OK. I've found it.\")\n",
    "        else:\n",
    "            start_epoch = 0\n",
    "            start_batch_id = 0\n",
    "            counter = 1\n",
    "            print(\" [i] NOTHING FOUND TO LOAD!\")\n",
    "\n",
    "        # loop for epoch\n",
    "        start_time = time.time()\n",
    "        for epoch in range(start_epoch, self.epoch):\n",
    "\n",
    "            # get batch data\n",
    "            for idx in range(start_batch_id, self.num_batches):\n",
    "                batch_images = self.data_X[idx*self.batch_size:(idx+1)*self.batch_size]\n",
    "                batch_codes = self.data_y[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "\n",
    "\n",
    "                #update Interpreter\n",
    "                _, summary_str, int_loss = self.sess.run([self.int_optim, self.int_sum, self.int_loss],\n",
    "                                                       feed_dict={self.inputs: batch_images, self.y: batch_codes})\n",
    "\n",
    "                # update Main Model\n",
    "                _, summary_str_mm, mm_loss = self.sess.run(\n",
    "                    [self.mm_optim, self.mm_sum, self.mm_loss],\n",
    "                    feed_dict={self.y: batch_codes, self.inputs: batch_images})\n",
    "\n",
    "\n",
    "                # display training status\n",
    "                counter += 1\n",
    "                print(\"Epoch: [%2d] [%4d/%4d] time: %4.4f, int_loss: %.8f,mm_loss: %.8f\" \\\n",
    "                      % (epoch, idx, self.num_batches, time.time() - start_time, int_loss, mm_loss))\n",
    "\n",
    "\n",
    "            start_batch_id = 0\n",
    "\n",
    "            # save model\n",
    "            self.save(self.checkpoint_dir, counter)\n",
    "\n",
    "        # save model for final step\n",
    "        self.save(self.checkpoint_dir, counter)\n",
    "\n",
    "    @property\n",
    "    def model_dir(self):\n",
    "        return \"{}_{}\".format(\n",
    "            self.model_name, self.dataset_name)\n",
    "\n",
    "    def save(self, checkpoint_dir, step):\n",
    "        checkpoint_dir = os.path.join(checkpoint_dir, self.model_dir, self.model_name)\n",
    "\n",
    "        if not os.path.exists(checkpoint_dir):\n",
    "            os.makedirs(checkpoint_dir)\n",
    "\n",
    "        self.saver.save(self.sess,os.path.join(checkpoint_dir, self.model_name+'.model'), global_step=step)\n",
    "\n",
    "    def load(self, checkpoint_dir):\n",
    "        import re\n",
    "        print(\" [i] Wait a sec...\")\n",
    "        checkpoint_dir = os.path.join(checkpoint_dir, self.model_dir, self.model_name)\n",
    "\n",
    "        ckpt = tf.train.get_checkpoint_state(checkpoint_dir)\n",
    "        if ckpt and ckpt.model_checkpoint_path:\n",
    "            ckpt_name = os.path.basename(ckpt.model_checkpoint_path)\n",
    "            self.saver.restore(self.sess, os.path.join(checkpoint_dir, ckpt_name))\n",
    "            counter = int(next(re.finditer(\"(\\d+)(?!.*\\d)\",ckpt_name)).group(0))\n",
    "            print(\" [i] OK. Reading Completed! {}\".format(ckpt_name))\n",
    "            return True, counter\n",
    "        else:\n",
    "            print(\" [i] NO CHECKPOINTS FOUND!\")\n",
    "            return False, 0\n",
    "    def just_load(self, tobe_tafsired):\n",
    "        \n",
    "        # initialize all variables\n",
    "        tf.compat.v1.global_variables_initializer().run()\n",
    "\n",
    "        # graph inputs for visualize training results\n",
    "        self.test_codes = self.data_y[0:self.batch_size]\n",
    "        self.sample_input = self.data_X[0:self.batch_size]\n",
    "\n",
    "\n",
    "        # saver to save model\n",
    "        self.saver = tf.compat.v1.train.Saver()\n",
    "\n",
    "\n",
    "        # restore check-point if it exits\n",
    "        could_load, checkpoint_counter = self.load(self.checkpoint_dir)\n",
    "        if could_load:\n",
    "            start_epoch = (int)(checkpoint_counter / self.num_batches)\n",
    "            start_batch_id = checkpoint_counter - start_epoch * self.num_batches\n",
    "            counter = checkpoint_counter\n",
    "            print(\" [i] Loading done!\")\n",
    "        samples, recon_error, code = self.sess.run(self.tafsir_images, feed_dict={self.inputs: tobe_tafsired})\n",
    "        return samples\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = []\n",
    "X_test = pd.read_csv(\"test.csv\")\n",
    "X_test = X_test/255\n",
    "X_test = X_test.values.reshape(-1,28,28,1)\n",
    "tests = X_test[0:64]\n",
    "with tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(allow_soft_placement=True)) as sess:\n",
    "# open session\n",
    "    pine = PINE(sess,\n",
    "                epoch=5,\n",
    "                batch_size=64,\n",
    "                dataset_name='mnist',\n",
    "                checkpoint_dir= 'checkpoint')\n",
    "\n",
    "    # build graph\n",
    "    pine.build_pine()\n",
    "    # initialize all variables\n",
    "    pine.train()\n",
    "    interprets = pine.just_load(tests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "fig, axs = plt.subplots(16, 8, figsize=(28,28))\n",
    "axs[0, 0].set_title('Input Image')\n",
    "axs[0, 2].set_title('Input Image')\n",
    "axs[0, 4].set_title('Input Image')\n",
    "axs[0, 6].set_title('Input Image')\n",
    "axs[0, 1].set_title('Interpretation')\n",
    "axs[0, 3].set_title('Interpretation')\n",
    "axs[0, 5].set_title('Interpretation')\n",
    "axs[0, 7].set_title('Interpretation')\n",
    "\n",
    "fig= plt.figure(figsize=(20, 20))\n",
    "\n",
    "for j in range(0,4):\n",
    "    for i in range(0,16):\n",
    "        tests_figs = tests[i+j*16].reshape((28,28))\n",
    "        axs[i, 2*j].imshow(tests_figs)\n",
    "        axs[i, 2*j].axis('off')\n",
    "\n",
    "        \n",
    "for j in range(0,4):\n",
    "    for i in range(0,16):\n",
    "        interprets_figs = interprets[i+j*16].reshape((28,28))\n",
    "        axs[i, 2*j+1].imshow(interprets_figs)\n",
    "        axs[i, 2*j+1].axis('off')        \n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
